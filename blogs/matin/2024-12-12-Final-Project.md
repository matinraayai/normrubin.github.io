---
author: matinraayai
format:
 html: default
title: Final Project: Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions
---
For the past year or so I've been working on a framework to instrument AMD GPU code objects at runtime, similar to [NVBit](https://github.com/NVlabs/NVBit) for NVIDIA GPUs and [GTPin](https://www.intel.com/content/www/us/en/developer/articles/tool/gtpin.html) for Intel GPUs. My final project is essentially a major feature that I wanted to implement in the framework. I think it will be helpful to briefly go over the framework and how it is designed to better understand my final project and the challenges faced when
implementing it.
## Background
Instrumentation, in a nutshell, entails the modification of a binary in some shape or form, with the goal of gaining a better understanding of how it works. The motivation behind instrumentation ranges from debugging and profiling (e.g. [Valgrind](https://valgrind.org/), [Sanatizers](https://github.com/google/sanitizers/wiki/AddressSanitizer)), to architectural research (e.g. recording the addresses accessed by a target workload to better design future caches).
Instrumentation has been done for quite some time on the CPU side; Some examples include Intel's [Pin](https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html), [DynInst](https://github.com/dyninst/dyninst), and [DynamoRio](https://dynamorio.org/). In recent years, this capability has been extended to NVIDIA GPUs via
[NVBit](https://github.com/NVlabs/NVBit) and to Intel GPUs via
[GTPin](https://www.intel.com/content/www/us/en/developer/articles/tool/gtpin.html). Both these frameworks are capable of "dynamic" instrumentation, which means they don't require access to the source code and modify the binary directly, just before the code runs.
"Static" instrumentation, on the other hand, instruments the binary "offline", and usually during compile time as part of the compilation process. Generally, dynamic instrumentation is preferred since it doesn't require the recompilation of the target code from scratch, which is impossible to do for closed-source applications. Another advantage of dynamic frameworks is the ability to switch between instrumented and original versions of the binary (also referred to as "selective instrumentation"), as they have access to both versions. This helps tremendously with reducing the overhead caused by instrumentation, which often times is very significant. Static instrumentation, on the other hand, can generated more efficient instrumented code as they have access to the compilation steps and analysis of the target code otherwise not available to dynamic framworks. They also don't incur a code-generation penalty compared to dynamic frameworks, as this step happens offline in static frameworks.

So far there hasn't been a successful attempt at creating a framework for dynamically instrumenting AMD GPU applications; Luthier (the framework that I've been working on as part of my PhD research), to the best of my (and my collegeues') knowledege, will be the first ever dynamic instrumentation framework targeting AMD GPUs. In the next section I go over the challenges I faced when designing Luthier, and then go over how it works and how it interfaces with AMD's hardware and software stack. Since this is a compiler class I will mostly focus on the compiler aspects of Luthier and I only briefly mention details on interfacing with the rest of the [ROCm](https://www.amd.com/en/products/software/rocm.html) stack. I will make these details available in the near future.

## Luthier: How It Was Designed, and How It Works
The initial design of Luthier was heavily inspired by NVBit. NVBit tools are `.so` shared objects that are loaded before the CUDA application using the [`LD_PRELOAD` trick](https://www.baeldung.com/linux/ld_preload-trick-what-is). The tools use [CUPTI](https://docs.nvidia.com/cupti/index.html) APIs to get notified when the CUDA application hit certain "events" (e.g. calls `cudaMalloc`, launches a kernel, etc.) through invoking a "callback" function defined inside the tool. NVBit tools use these callbacks to intercept CUDA kernels before they are launched, and afterwards inspect them using [`nvdisasm`](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html). Luthier tools works similiar to NVBit tools: they are shared objects loaded using the `LD_PRELOAD` environment variable; To be able to intercept and inspect kernels we designed a similar mechanism using [`rocprofiler-sdk`](https://github.com/ROCm/rocprofiler-sdk/) to notify tools about key events in the application. We then use the ROCr (HSA) runtime APIs to locate where the kernel has been loaded on the target GPU device, and even the ELF (i.e. code object) this kernel was loaded from.

But this was where we ran into the following issues:
1. **We realized that instrumenting AMDGPU code objects the "NVBit way" is not feasible or at best, very hard to implement**: NVBit instruments GPU applications by replacing the target instruction with a `JMP` to a trampoline region. The trampoline then spills the application's register onto the thread's stack, sets up the arguments to a instrumentation device function call, and the proceeds to call it. After returning, the trampoline will restore the registers, execute the original instruction, and then jumps back to the original kernel. This design is successful because SASS (NVIDIA GPU's hardware assembly) instructions have a fixed size, meaning that it's very easy to replace a single instruction with a "long jump" to almost any virtual memory address on the device, all without changing the layout of the code. Not changing the code layout is very important in dynamic instrumentation, as it ensures indirect jumps/calls won't break. This is not the case on AMD GPUs, as CDNA GPUs have a 4-byte short jump, only covering $2^{18}$ bytes, and an 8-byte long jump. To make matters worse, the long jump requires additional instructions to load the jump target into registers. We can argue that we might be able to make the NVBit trampoline work by allocating fixed-address executable memory using a custom allocator (which NVBit seem to also have),  but this goes completely against ROCr's APIs of asking for executable regions of memory using the `hsa_executable_t` interfaces, and is very hard to implement and manage. Even then this is only a temporary fix, as the aggregation of trampoline logic for each instruction will quickly go over the range managable by the short jump instruction. **This meant we needed a completely different approach for instrumentation**.
2. **Reusing Dead Registers**: NVBit inserts calls to pre-compiled instrumentation device functions with a set calling convention, which NVBit has no choice but to obey by spilling/restoring a large number of thread registers at all times. Accessing the stack is not cheap on GPUs, hence **we wanted to find a way to adapt the same instrumentation function to each instrumentation point to reuse dead registers** in order to speed up instrumented code.
3. **Allocating And Accessing Scratch (Local) Memory Is Not Trivial on AMD GPUs**: in the NVBit paper, accessing the stack (local memory) of each thread for spilling and restoring registers is mentioned; However, how this access is done without interfering with the application's local memory is not explained. We assume that NVBit assumes all SASS code adhere to NVIDIA's calling conventions, which is enforced via their virtual PTX ISA; Hence, it can always have a stack pointer which allows for interleaving the spilled registers with the application's local memory. AMD GPUs, however, can be programmed directly using hardware assembly, and have multiple calling conventions that don't enforce presence of a stack pointer register. **We needed to find a way to access the stack for instrumentation without the presnce of a stack pointer**.
4. **Requesting access to scratch might displace the SGPR arguments**: According to the [AMDGPU LLVM Docs](https://llvm.org/docs/AMDGPUUsage.html), when a kernel is launched, a set of wavefront-common values are loaded into SGPRs for use by the kernel. This includes the address of the kernel argument buffer or the address of the queue (i.e. command processor) used to launch the kernel. It also includes resources needed to access local/scratch memory in each thread. These resource must be explicitly requested by the kernel, and [this table](https://llvm.org/docs/AMDGPUUsage.html#amdgpu-amdhsa-sgpr-register-set-up-order-table) shows the order these SGPRs are setup. One thing to note is that accessing scratch requires access to the "Private Segment Buffer Descriptor" and/or "Flat Scratch Address" of the queue used to launch the kernel, as well as a "wave offset", which is the offset from the queue's scratch address to the current wavefront's scratch space. This makes accessing scratch in instrumentation functions particularly challenging, especially for applications that don't require scratch and don't set it up. Luthier **must take into account these shift in SGPR arguments and must emit code to set it up for the instrumentation stack and then move the SGPR arguments to their original place**. It also have to **store the scratch information somewhere to be able to access it later in the instrumentation routines**.

Luthier addresses each of the afformentioned challenge as follows:
1. Instead of carrying out instrumentation by directly modifying the loaded code on the GPU, Luthier opts to create a "standalone executable", which contains the original code as well as the instrumentation logic. The standalone executable will use ROCr dynamic loader's features to link against the "static global variables" already loaded in the original code (e.g. variables annotated with `__device__` or `__managed__` in HIP code). This respects the ROCr APIs the most, not requiring any significant changes to the low level runtime.
2. Instead of calling pre-compiled device functions, Luthier embeds the optimized LLVM IR bitcode of the device instrumentation logic in its device code objects at tool compile time. This ensures that the HIP runtime will load the bitcode for free, allowing the Luthier runtime to do more optimizations on the instrumentation logic and adapt it to each instrumentation point.
3. (and 4.) Luthier defines a new concept called the "State Value Array (SVA)", an array of 64 32-bit values that can be loaded into a single VGPR of a single wavefront (each wavefront has 64 threads). SVA is in charge of storing the instrumentation stack information, which is always allocated on top of what the application requested originally. It also keeps track of other wavefront-specific values. SVA is setup using a "Kernel preamble code", which reads the SGPR arguments and saves them into the SVA's VGPR lanes, right before reverting the SGPR arguments back to their original formation before the target kernel starts executing. SVA is stored in the applications' dead/unused registers. In most recent hardware, the SVA can be either stored in a single A/VGPR, or spilled on a static point of the kernel stack with a single SGPR pointing to it so it can be loaded later using `SCRATCH` instructions.

Luthier implements these designs by heavily leveraging the LLVM project and its AMDGPU backend, which we explain in more detail below:

### Inspecting Code Objects
As we mentioned earlier, Luthier instruments code by duplicating the target application's code so that it can freely inject instrumentation logic inside it. To do this, Luthier takes in a single ELF/executable, and inspects its symbols using LLVM's object utilities. It then identifies the kernels and device functions inside the ELF and disassembles them into LLVM MC instructions. [MC](https://blog.llvm.org/2010/04/intro-to-llvm-mc-project.html) is the machine code/assembler layer of LLVM. It is meant to represent "physical" instructions and registers. While disassembling code, Luthier identifies the branch instructions and identifies their targets if possible for later use.

After disassembly is complete Luthier uses the obtained information from the ELF to "lift" it to LLVM Machine IR (MIR). [MIR](https://llvm.org/docs/CodeGenerator.html) is LLVM's representation used in its target-independent code generator (i.e. backends). It is a superset of LLVM MC, meaning an LLVM MC instruction can also be easily converted to
an LLVM MIR instruction by reusing the same enums for opcodes and registers. The reason behind lifting to LLVM MIR is as follows:
1. MIR has very convenient ways for iterating over the code and querying properties regarding the code and the instructions which is otherwise costly to implement ourselves.
2. MIR's high-level utilities allow for removing/adding things to the ELF otherwise not possible with ELF-modification frameworks e.g. [ELFIO](https://github.com/serge1/ELFIO/tree/main). This makes it easy to modify the kernel specifications and removing symbols that are not used in the target kernel and are unrelated.
3. The compiler machine passes only operate on the MIR representation. Lifting to MIR makes it easier to re-use analysis already available in LLVM's code generator or add new ones.

LLVM MIR consists of a set of "pseudo" instructions that is equivalent to a set of "physical", target-specific instructions in MC; For example, `S_ADD_U32` is a pseudo instruction in MIR which maps to `S_ADD_U32_vi`, `S_ADD_32_gfx12`, and so on. The same goes for some registers e.g. `TTMP` or `M0`. The primary reason for this indirection is to allow for different encoding/decodings on different targets. Although in theory, one should be able to use both pseudo and target-specific opcodes in the MIR, the AMDGPU backend primarily expects the pseudo variant to be present in the code; Hence during its tool library compilation, Luthier uses LLVM's [TableGen](https://llvm.org/docs/TableGen/) to read over all the opcodes/registers and creates an inverse mapping between target-specific opcode and their pseudo equivalents. Luthier then uses this table to convert the opcode and registers to their pseudo variants during runtime.

Just like [LLVM Bolt](https://github.com/llvm/llvm-project/blob/main/bolt/README.md), Luthier requires the inspected ELFs to have full relocation information. This way, it is able to correctly lift things like reading the address of a symbol:
```asm
s_getpc_b64 s[4:5]
s_add_u32 s4, s4, func1@rel32@lo+4
s_addc_u32 s5, s5, func1@rel32@lo+4
```

The result of the lifting process it a `LiftedRepresentation`, which is a mapping between the HSA/ROCr symbols of the ELF and their LLVM equivalent.

### Generating Instrumentation Code
As mentioned earlier,

### Step 1. Intercepting AMD GPU Code Objects

### Step 2. Analyzing the

## New Feature: Lowering AMDGPU
On NVIDIA GPUs, accessing this property is very easy; There are dedicated registers the hardware can query to obtain these values; On AMD GPUs, however, there are no dedicated registers for these; Instead, they are either passed as arguments to the SGPRs or

I implemented